command:
  - ${env}
  - venv/Scripts/python.exe
  - ${program}
  - ${args}
method: bayes
project: detect-sleep-states
program: main.py
name: sweep-transformer-gru
metric:
  goal: maximize
  name: cv_score
parameters:
  hpo_model:
    parameters:
      name:
        value: transformer_gru_sweep
      data_info:
        parameters:
          downsampling_factor:
            values:
              - 12
              - 24
          latitude:
            value: 40.73061
          longitude:
            value: -73.935242
          window_size:
            value: 17280
      preprocessing:
        value: [
          {
              "kind": "mem_reduce",
              "id_encoding_path": "series_id_encoding.json"
          },
          {
              "kind": "similarity_nan",
              "as_feature": true
          },
          {
              "kind": "add_state_labels",
              "id_encoding_path": "series_id_encoding.json",
              "events_path": "data/raw/train_events.csv",
              "use_similarity_nan": true,
              "fill_limit": 8640,
              "nan_tolerance_window": 5
          },
          {
              "kind": "add_event_labels",
              "id_encoding_path": "series_id_encoding.json",
              "events_path": "data/raw/train_events.csv",
              "smoothing": 0,
              "steepness": 2
          },
          {
              "kind": "split_windows"
          }]
      feature_engineering:
        value: [
          {
              "kind": "time",
              "time_features": ["hour","minute"]
          },
          {
              "kind": "sun",
              "sun_features": ["azimuth", "altitude"]
          }]
      pretraining:
        parameters:
          downsample:
            parameters:
              features:
                value: ["f_anglez", "f_enmo"]
              methods:
                value: ["min", "max", "mean", "std", "median", "range", "var"]
              standard:
                value: "mean"
          test_size:
            value: 0.2
          scaler:
            parameters:
              kind:
                value: "standard-scaler"
              copy:
                value: true
      architecture:
        parameters:
          activation_delay:
            max: 50
            min: 20
          batch_size:
            max: 128
            min: 64
          early_stopping:
            max: 50
            min: 15
          epochs:
            value: 250
          loss:
            values:
              - mse-torch
              - shrinkage-loss
              - kldiv-torch
          lr:
            max: 0.03
            min: 0.005
          lr_schedule:
            parameters:
              cycle_limit:
                values:
                - 2
                - 3
                - 4
              lr_min:
                value: 0.002
              t_initial:
                max: 45
                min: 25
              warmup_lr_init:
                value: 1e-06
              warmup_t:
                max: 7
                min: 2
          mask_unlabeled:
            values:
              - true
              - false
          network_params:
            parameters:
              heads:
                max: 8
                min: 3
              emb_dim:
                max: 256
                min: 64
              forward_dim:
                max: 1024
                min: 256
              n_layers:
                max: 10
                min: 5
              pooling:
                value: "gru"
              pooling_args:
                parameters:
                  dropout:
                    value: 0.0
                  bidir:
                    values:
                    - true
                    - false
                  hidden_size:
                    max: 60
                    min: 6
                  n_layers:
                    max: 10
                    min: 5
                  activation:
                    values:
                    - relu
                    - gelu
              tokenizer:
                value: "patch"
              tokenizer_args:
                parameters:
                  patch_size:
                    values:
                    - 4
                    - 6
                    - 8
                    - 12
                    - 16
              pe:
                value: "fixed"
              dropout:
                max: 0.9
                min: 0
          optimizer:
            values:
              - adam-torch
              - adamw-torch
          threshold:
            value: 0
          type:
            value: event-segmentation-transformer
